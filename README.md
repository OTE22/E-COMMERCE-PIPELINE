# ğŸ›’ E-Commerce Analytics Platform

[![GitHub](https://img.shields.io/badge/GitHub-OTE22%2FE--COMMERCE--PIPELINE-blue?logo=github)](https://github.com/OTE22/E-COMMERCE-PIPELINE)
[![Author](https://img.shields.io/badge/Author-Ali%20Abbass-purple)](https://github.com/OTE22)

A **production-grade, real-time analytics platform** for e-commerce businesses. Built with FastAPI, PostgreSQL, Redis, Kafka, and modern data engineering practices.

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success)
![Tests](https://img.shields.io/badge/Tests-45%20Files%20Verified-brightgreen)

---

> [!IMPORTANT]
> ## ğŸš€ Before You Deploy
> 
> **Code Status**: âœ… All 45 Python files verified with no syntax errors
> 
> ### Pre-Deployment Checklist:
> 1. **Clone the repository** to your server
> 2. **Configure `.env`** with your production credentials:
>    - `DATABASE_HOST`, `DATABASE_PASSWORD` (use AWS RDS)
>    - `REDIS_URL` (use AWS ElastiCache or local Redis)
>    - `SECRET_KEY` (generate a secure random key)
> 3. **Set production mode**: `APP_ENV=production`, `DEBUG=false`
> 4. **Run migrations**: `alembic upgrade head`
> 5. **Enable SSL/TLS** using Let's Encrypt or AWS ACM
> 
> ### Quick Deploy to AWS EC2:
> ```bash
> sudo git clone https://github.com/OTE22/E-COMMERCE-PIPELINE.git /opt/ecommerce-analytics
> cd /opt/ecommerce-analytics
> sudo chmod +x scripts/deploy.sh
> sudo ./scripts/deploy.sh --first-time
> ```
> 
> ğŸ“– Full deployment guide: [docs/AWS_EC2_DEPLOYMENT.md](docs/AWS_EC2_DEPLOYMENT.md)

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Quick Start](#-quick-start)
- [API Documentation](#-api-documentation)
- [Frontend Dashboard](#-frontend-dashboard)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [Testing](#-testing)
- [CI/CD Pipeline](#-cicd-pipeline)
- [Production Deployment](#-production-deployment)
- [Monitoring](#-monitoring)
- [Contributing](#-contributing)

---

## âœ¨ Features

### Data Ingestion
- **Batch Processing**: CSV, JSON, Parquet file loaders with schema validation
- **Stream Processing**: Kafka consumers with exactly-once semantics
- **Dead Letter Queue**: Failed message handling with retry logic

### Data Transformation
- **ETL Pipeline**: Cleaning, normalization, and enrichment
- **Star Schema**: Optimized fact/dimension tables for analytics
- **RFM Analysis**: Recency, Frequency, Monetary customer scoring

### Data Quality
- **Validation Framework**: Null checks, range validation, pattern matching
- **Anomaly Detection**: Z-score, IQR, percentage change algorithms
- **Alerting**: Slack, email, PagerDuty integration ready

### API & Serving
- **REST API**: Full CRUD with pagination, filtering, caching
- **GraphQL**: Flexible queries with Strawberry GraphQL
- **Redis Caching**: Namespace-based cache managers

### Analytics & ML
- **Feature Engineering**: Customer churn, CLV, recommendations
- **ML Datasets**: Train/test splits with metadata
- **Real-time Dashboards**: Interactive visualizations

### Security
- **PII Handling**: Hashing and masking for GDPR compliance
- **Rate Limiting**: Per-client request throttling
- **Security Headers**: CORS, XSS, CSRF protection

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  CSV/    â”‚  â”‚  Kafka   â”‚  â”‚  APIs    â”‚  â”‚  S3/     â”‚       â”‚
â”‚   â”‚  JSON    â”‚  â”‚  Events  â”‚  â”‚          â”‚  â”‚  GCS     â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚             â”‚             â”‚
         â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INGESTION LAYER                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚    Batch Loader      â”‚  â”‚   Stream Consumer    â”‚            â”‚
â”‚   â”‚  (Prefect Orchestr.) â”‚  â”‚   (Kafka + DLQ)      â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                         â”‚
               â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRANSFORMATION LAYER                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ Cleaners â”‚  â”‚Enrichers â”‚  â”‚Validatorsâ”‚  â”‚ Anomaly  â”‚       â”‚
â”‚   â”‚          â”‚  â”‚(RFM,CLV) â”‚  â”‚          â”‚  â”‚ Detector â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚             â”‚             â”‚             â”‚
         â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STORAGE LAYER                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚     PostgreSQL       â”‚  â”‚       Redis          â”‚            â”‚
â”‚   â”‚   (Star Schema)      â”‚  â”‚     (Cache)          â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                         â”‚
               â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SERVING LAYER                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ REST API â”‚  â”‚ GraphQL  â”‚  â”‚ Frontend â”‚  â”‚ Superset â”‚       â”‚
â”‚   â”‚ FastAPI  â”‚  â”‚Strawberryâ”‚  â”‚Dashboard â”‚  â”‚ Dashbd   â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Tech Stack

| Category | Technology |
|----------|------------|
| **Language** | Python 3.11+ |
| **API Framework** | FastAPI + Uvicorn/Gunicorn |
| **Database** | PostgreSQL 15+ |
| **Cache** | Redis 7+ |
| **Streaming** | Apache Kafka |
| **Orchestration** | Prefect |
| **Data Processing** | Polars, Pandas |
| **ML** | Scikit-learn, NumPy |
| **Dashboards** | Apache Superset |
| **Monitoring** | Prometheus + Grafana |
| **Containerization** | Docker + Docker Compose |

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Docker & Docker Compose
- Git

### 1. Clone Repository
```bash
git clone <repository-url>
cd E-COMMERCE
```

### 2. Set Up Environment
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure Environment
```bash
# Copy example environment file
cp .env.example .env

# Edit with your settings
# Required: DATABASE_URL, REDIS_URL
```

### 4. Start Services
```bash
# Start all services with Docker
docker-compose up -d

# Or start individual services
docker-compose up -d postgres redis
```

### 5. Initialize Database
```bash
# Run migrations
alembic upgrade head
```

### 6. Generate Sample Data
```bash
python -m src.data.generators
```

### 7. Start API Server

**Development Mode:**
```bash
python run_server.py --dev
```

**Production Mode:**
```bash
# With Uvicorn
python run_server.py

# With Gunicorn (recommended)
python run_server.py --gunicorn

# Or directly
gunicorn src.main:app -c gunicorn.conf.py
```

### 8. Access Services
| Service | URL |
|---------|-----|
| **Frontend Dashboard** | http://localhost:8000 |
| **API Documentation** | http://localhost:8000/docs |
| **GraphQL Playground** | http://localhost:8000/graphql |
| **Superset** | http://localhost:8088 |
| **Prometheus** | http://localhost:9090 |
| **Grafana** | http://localhost:3000 |

---

## ğŸ“š API Documentation

### REST API Endpoints

#### Health Check
```http
GET /api/v1/health
GET /api/v1/health/live
GET /api/v1/health/ready
```

#### Orders
```http
GET    /api/v1/orders                    # List orders (paginated)
GET    /api/v1/orders/{id}               # Get order details
GET    /api/v1/orders/metrics            # Order metrics
GET    /api/v1/orders/customer/{id}      # Customer orders
```

**Query Parameters:**
- `page` (int): Page number (default: 1)
- `page_size` (int): Items per page (default: 50, max: 100)
- `status` (str): Filter by status
- `start_date` / `end_date` (date): Date range filter

#### Products
```http
GET    /api/v1/products                  # List products
GET    /api/v1/products/{id}             # Product details
GET    /api/v1/products/metrics          # Catalog metrics
GET    /api/v1/products/categories       # Category list
GET    /api/v1/products/low-stock        # Low stock alerts
```

#### Customers
```http
GET    /api/v1/customers                 # List customers
GET    /api/v1/customers/{id}            # Customer details
GET    /api/v1/customers/metrics         # Customer metrics
GET    /api/v1/customers/segments        # Segment distribution
GET    /api/v1/customers/top-spenders    # Top customers by LTV
GET    /api/v1/customers/at-risk         # Churn risk customers
```

#### Analytics
```http
GET    /api/v1/analytics/sales/overview  # Sales overview
GET    /api/v1/analytics/sales/trend     # Revenue trend
GET    /api/v1/analytics/sales/by-category  # Category breakdown
GET    /api/v1/analytics/sales/hourly    # Hourly distribution
GET    /api/v1/analytics/customers/cohorts  # Cohort analysis
GET    /api/v1/analytics/products/top-selling  # Top products
```

### GraphQL

Access the GraphQL Playground at `/graphql`

**Example Query:**
```graphql
query {
  orders(pagination: {limit: 10}) {
    orderId
    orderNumber
    totalAmount
    status
    customer {
      customerId
      segment
      lifetimeValue
    }
  }
}
```

---

## ğŸ¨ Frontend Dashboard

The platform includes a modern analytics dashboard built with vanilla HTML/CSS/JavaScript.

### Features
- **KPI Cards**: Revenue, orders, customers, AOV with trends
- **Charts**: Revenue trends, category breakdown, hourly distribution
- **Tables**: Top products, recent orders, customer list
- **Dark Theme**: Premium dark mode with glassmorphism effects
- **Responsive**: Works on desktop, tablet, and mobile

### Accessing the Dashboard
Navigate to `http://localhost:8000` after starting the server.

---

## ğŸ“ Project Structure

```
E-COMMERCE/
â”œâ”€â”€ src/                        # Application source code
â”‚   â”œâ”€â”€ config/                 # Configuration management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py         # Pydantic settings
â”‚   â”œâ”€â”€ database/               # Database layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection.py       # Async connection pool
â”‚   â”‚   â””â”€â”€ models.py           # SQLAlchemy models
â”‚   â”œâ”€â”€ ingestion/              # Data ingestion
â”‚   â”‚   â”œâ”€â”€ batch_loader.py     # Batch file processing
â”‚   â”‚   â””â”€â”€ stream_consumer.py  # Kafka consumer
â”‚   â”œâ”€â”€ transformation/         # ETL transformations
â”‚   â”‚   â”œâ”€â”€ cleaners.py         # Data cleaning
â”‚   â”‚   â”œâ”€â”€ enrichers.py        # Data enrichment
â”‚   â”‚   â””â”€â”€ transformers.py     # Pipeline orchestration
â”‚   â”œâ”€â”€ quality/                # Data quality
â”‚   â”‚   â”œâ”€â”€ validators.py       # Validation framework
â”‚   â”‚   â””â”€â”€ anomaly_detector.py # Anomaly detection
â”‚   â”œâ”€â”€ serving/                # API layer
â”‚   â”‚   â”œâ”€â”€ cache.py            # Redis caching
â”‚   â”‚   â””â”€â”€ api/                # FastAPI routes
â”‚   â”‚       â”œâ”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ middleware.py
â”‚   â”‚       â””â”€â”€ graphql.py
â”‚   â”œâ”€â”€ ml/                     # ML feature engineering
â”‚   â”‚   â”œâ”€â”€ features.py         # Feature computation
â”‚   â”‚   â””â”€â”€ datasets.py         # Dataset builder
â”‚   â”œâ”€â”€ data/                   # Data utilities
â”‚   â”‚   â””â”€â”€ generators.py       # Synthetic data
â”‚   â””â”€â”€ main.py                 # Application entry point
â”œâ”€â”€ frontend/                   # Dashboard UI
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ css/styles.css
â”‚   â””â”€â”€ js/
â”œâ”€â”€ workflows/                  # Prefect ETL workflows
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ infrastructure/             # Infrastructure configs
â”‚   â””â”€â”€ docker/
â”œâ”€â”€ alembic/                    # Database migrations
â”œâ”€â”€ docker-compose.yml          # Docker services
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ gunicorn.conf.py           # Production server config
â”œâ”€â”€ run_server.py              # Server entry point
â””â”€â”€ README.md                  # This file
```

---

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file based on `.env.example`:

```env
# Application
APP_ENV=development
DEBUG=true

# Database
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=ecommerce_analytics
DATABASE_USER=ecommerce
DATABASE_PASSWORD=your_secure_password

# Redis
REDIS_URL=redis://localhost:6379/0

# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# API
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# Security
SECRET_KEY=your-secret-key-here
```

See `.env.example` for all available options.

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run unit tests only
pytest tests/unit/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/unit/test_transformation.py -v
```

---

## ğŸ”„ CI/CD Pipeline

This project uses **GitHub Actions** for continuous integration and deployment.

### Workflows

| Workflow | Trigger | Description |
|----------|---------|-------------|
| **CI** | Push/PR to `main`, `develop` | Lint, test, security scan, Docker build |
| **CD** | Push to `main`, version tags | Deploy to staging/production |
| **Scheduled** | Daily 2 AM UTC | Data quality, backups, reports |

### CI Pipeline (`.github/workflows/ci.yml`)

```yaml
Jobs:
  1. lint        â†’ Ruff, Black, isort, MyPy
  2. test        â†’ Pytest with PostgreSQL & Redis
  3. security    â†’ Bandit, Safety dependency check
  4. build       â†’ Docker image + Trivy scan
```

### CD Pipeline (`.github/workflows/cd.yml`)

```yaml
Jobs:
  1. build-and-push    â†’ Push to GitHub Container Registry
  2. deploy-staging    â†’ Auto-deploy on main branch
  3. deploy-production â†’ Deploy on version tags (v*)
  4. migrate           â†’ Run Alembic migrations
```

### Required Secrets

Configure these in GitHub repository settings:

| Secret | Description |
|--------|-------------|
| `DATABASE_URL` | Production database connection string |
| `STAGING_DATABASE_URL` | Staging database connection |
| `AWS_ACCESS_KEY_ID` | For S3 backups (optional) |
| `AWS_SECRET_ACCESS_KEY` | For S3 backups (optional) |

### Dependabot

Automatic dependency updates configured for:
- Python packages (weekly)
- GitHub Actions (weekly)
- Docker base images (weekly)

### Branch Protection

Recommended settings for `main` branch:
- âœ… Require pull request reviews
- âœ… Require status checks to pass (CI)
- âœ… Require branches to be up to date

---

## ğŸš¢ Production Deployment

### AWS EC2 Quick Deploy

```bash
# 1. SSH into your EC2 instance
ssh -i your-key.pem ubuntu@your-ec2-ip

# 2. Clone and deploy
sudo git clone https://github.com/your-repo/ecommerce-analytics.git /opt/ecommerce-analytics
cd /opt/ecommerce-analytics
sudo chmod +x scripts/deploy.sh
sudo ./scripts/deploy.sh --first-time

# 3. Configure environment
sudo nano .env  # Update database credentials

# 4. Restart
sudo docker-compose -f docker-compose.prod.yml up -d
```

### AWS Architecture (Recommended)

| Component | AWS Service |
|-----------|-------------|
| **Compute** | EC2 t3.large (or ECS Fargate) |
| **Database** | RDS PostgreSQL |
| **Cache** | ElastiCache Redis |
| **Load Balancer** | ALB (Application Load Balancer) |
| **SSL** | ACM (Certificate Manager) |
| **Storage** | S3 (for data lake) |
| **Monitoring** | CloudWatch |

### Docker Deployment
```bash
# Development
docker-compose up -d

# Production (optimized)
docker-compose -f docker-compose.prod.yml up -d --build

# Scale API workers
docker-compose up -d --scale api=3
```

### Manual Deployment
```bash
pip install -r requirements.txt
alembic upgrade head
gunicorn src.main:app -c gunicorn.conf.py
```

### Production Checklist
- [ ] Set `APP_ENV=production` and `DEBUG=false`
- [ ] Configure secure `SECRET_KEY`
- [ ] Use RDS instead of local PostgreSQL
- [ ] Use ElastiCache instead of local Redis
- [ ] Enable SSL/TLS (Let's Encrypt or ACM)
- [ ] Configure Security Groups properly
- [ ] Set up CloudWatch alarms
- [ ] Enable automated backups
- [ ] Configure auto-scaling (optional)

ğŸ“– **Full AWS deployment guide**: [docs/AWS_EC2_DEPLOYMENT.md](docs/AWS_EC2_DEPLOYMENT.md)

---

## ğŸ“Š Monitoring

### Prometheus Metrics
Available at `/metrics`:
- Request duration histograms
- Error rates by endpoint
- Cache hit/miss ratios
- Database connection pool stats

### Grafana Dashboards
Pre-configured dashboards for:
- API performance
- Database metrics
- Redis cache stats
- Business KPIs

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- FastAPI for the excellent web framework
- Chart.js for beautiful visualizations
- Apache Kafka for reliable streaming
- The open-source community

---

**Built with â¤ï¸ for e-commerce analytics**
